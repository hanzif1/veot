INFO 12-20 12:52:03 api_server.py:585] vLLM API server version 0.6.4.post1
INFO 12-20 12:52:03 api_server.py:586] args: Namespace(host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['grounder-vl-72b'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 12-20 12:52:03 api_server.py:175] Multiprocessing frontend to use ipc:///tmp/8d608083-f84a-4109-b7f8-4bd6f1a65002 for IPC Path.
INFO 12-20 12:52:03 api_server.py:194] Started engine process with PID 807284
INFO 12-20 12:52:10 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 12-20 12:52:10 config.py:1020] Defaulting to use mp for distributed inference
WARNING 12-20 12:52:10 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 12-20 12:52:13 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 12-20 12:52:13 config.py:1020] Defaulting to use mp for distributed inference
WARNING 12-20 12:52:13 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 12-20 12:52:13 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='/HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct', speculative_config=None, tokenizer='/HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=grounder-vl-72b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
WARNING 12-20 12:52:13 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-20 12:52:13 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 12-20 12:52:14 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 12:52:14 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 12:52:14 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 12:52:14 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 12:52:14 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 12:52:14 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 12:52:14 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 12:52:17 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 12:52:17 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 12:52:17 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 12:52:17 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 12-20 12:52:17 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 12:52:17 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 12:52:17 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 12-20 12:52:17 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 12-20 12:52:20 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /HOME/nsccgz_zgchen/nsccgz_zgchen_6/.cache/vllm/gpu_p2p_access_cache_for_3,4,5,6.json
INFO 12-20 12:52:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /HOME/nsccgz_zgchen/nsccgz_zgchen_6/.cache/vllm/gpu_p2p_access_cache_for_3,4,5,6.json
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 12:52:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /HOME/nsccgz_zgchen/nsccgz_zgchen_6/.cache/vllm/gpu_p2p_access_cache_for_3,4,5,6.json
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 12:52:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /HOME/nsccgz_zgchen/nsccgz_zgchen_6/.cache/vllm/gpu_p2p_access_cache_for_3,4,5,6.json
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 12:52:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /HOME/nsccgz_zgchen/nsccgz_zgchen_6/.cache/vllm/gpu_p2p_access_cache_for_3,4,5,6.json
INFO 12-20 12:52:35 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x148236904bc0>, local_subscribe_port=51637, remote_subscribe_port=None)
INFO 12-20 12:52:35 model_runner.py:1072] Starting to load model /HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct...
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 12:52:35 model_runner.py:1072] Starting to load model /HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct...
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 12:52:35 model_runner.py:1072] Starting to load model /HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct...
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 12:52:35 model_runner.py:1072] Starting to load model /HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct...
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808074)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808076)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=808075)[0;0m WARNING 12-20 12:52:36 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
Loading safetensors checkpoint shards:   0% Completed | 0/38 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/38 [00:18<11:10, 18.11s/it]
Loading safetensors checkpoint shards:   5% Completed | 2/38 [00:33<09:44, 16.24s/it]
Loading safetensors checkpoint shards:   8% Completed | 3/38 [01:01<12:45, 21.87s/it]
Loading safetensors checkpoint shards:  11% Completed | 4/38 [01:06<08:30, 15.01s/it]
Loading safetensors checkpoint shards:  13% Completed | 5/38 [01:28<09:45, 17.75s/it]
Loading safetensors checkpoint shards:  16% Completed | 6/38 [01:45<09:17, 17.43s/it]
Loading safetensors checkpoint shards:  18% Completed | 7/38 [01:59<08:26, 16.34s/it]
Loading safetensors checkpoint shards:  21% Completed | 8/38 [02:14<07:53, 15.78s/it]
Loading safetensors checkpoint shards:  24% Completed | 9/38 [02:17<05:41, 11.77s/it]
Loading safetensors checkpoint shards:  26% Completed | 10/38 [02:32<06:02, 12.96s/it]
Loading safetensors checkpoint shards:  29% Completed | 11/38 [02:55<07:06, 15.79s/it]
Loading safetensors checkpoint shards:  32% Completed | 12/38 [03:00<05:31, 12.77s/it]
Loading safetensors checkpoint shards:  34% Completed | 13/38 [03:17<05:49, 14.00s/it]
Loading safetensors checkpoint shards:  37% Completed | 14/38 [03:34<05:58, 14.94s/it]
Loading safetensors checkpoint shards:  39% Completed | 15/38 [03:49<05:43, 14.95s/it]
Loading safetensors checkpoint shards:  42% Completed | 16/38 [03:56<04:32, 12.37s/it]
Loading safetensors checkpoint shards:  45% Completed | 17/38 [04:10<04:30, 12.87s/it]
Loading safetensors checkpoint shards:  47% Completed | 18/38 [04:26<04:40, 14.02s/it]
Loading safetensors checkpoint shards:  50% Completed | 19/38 [04:34<03:48, 12.01s/it]
Loading safetensors checkpoint shards:  53% Completed | 20/38 [04:43<03:20, 11.11s/it]
Loading safetensors checkpoint shards:  55% Completed | 21/38 [05:29<06:07, 21.63s/it]
Loading safetensors checkpoint shards:  58% Completed | 22/38 [06:07<07:03, 26.44s/it]
Loading safetensors checkpoint shards:  61% Completed | 23/38 [06:27<06:11, 24.76s/it]
Loading safetensors checkpoint shards:  63% Completed | 24/38 [06:58<06:11, 26.56s/it]
Loading safetensors checkpoint shards:  66% Completed | 25/38 [07:20<05:26, 25.14s/it]
Loading safetensors checkpoint shards:  68% Completed | 26/38 [07:37<04:33, 22.81s/it]
Loading safetensors checkpoint shards:  71% Completed | 27/38 [07:50<03:35, 19.63s/it]
Loading safetensors checkpoint shards:  74% Completed | 28/38 [08:07<03:08, 18.84s/it]
Loading safetensors checkpoint shards:  76% Completed | 29/38 [08:23<02:43, 18.12s/it]
Loading safetensors checkpoint shards:  79% Completed | 30/38 [08:37<02:15, 16.98s/it]
Loading safetensors checkpoint shards:  82% Completed | 31/38 [08:49<01:47, 15.32s/it]
Loading safetensors checkpoint shards:  84% Completed | 32/38 [08:55<01:16, 12.67s/it]
Loading safetensors checkpoint shards:  87% Completed | 33/38 [09:00<00:50, 10.17s/it]
Loading safetensors checkpoint shards:  89% Completed | 34/38 [09:15<00:46, 11.66s/it]
Loading safetensors checkpoint shards:  92% Completed | 35/38 [09:30<00:37, 12.65s/it]
Loading safetensors checkpoint shards:  95% Completed | 36/38 [09:45<00:26, 13.34s/it]
Loading safetensors checkpoint shards:  97% Completed | 37/38 [10:00<00:13, 13.82s/it]
Loading safetensors checkpoint shards: 100% Completed | 38/38 [10:15<00:00, 14.25s/it]
Loading safetensors checkpoint shards: 100% Completed | 38/38 [10:15<00:00, 16.19s/it]

[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:02:52 model_runner.py:1077] Loading model weights took 34.3156 GB
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:02:52 model_runner.py:1077] Loading model weights took 34.3156 GB
INFO 12-20 13:02:52 model_runner.py:1077] Loading model weights took 34.3156 GB
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:02:52 model_runner.py:1077] Loading model weights took 34.3156 GB
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:03:09 worker.py:232] Memory profiling results: total_gpu_memory=79.32GiB initial_memory_usage=37.00GiB peak_torch_memory=35.38GiB memory_usage_post_profile=39.10GiB non_torch_memory=4.75GiB kv_cache_size=35.23GiB gpu_memory_utilization=0.95
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:03:09 worker.py:232] Memory profiling results: total_gpu_memory=79.32GiB initial_memory_usage=36.77GiB peak_torch_memory=35.38GiB memory_usage_post_profile=38.63GiB non_torch_memory=4.28GiB kv_cache_size=35.70GiB gpu_memory_utilization=0.95
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:03:09 worker.py:232] Memory profiling results: total_gpu_memory=79.32GiB initial_memory_usage=37.00GiB peak_torch_memory=35.38GiB memory_usage_post_profile=39.10GiB non_torch_memory=4.75GiB kv_cache_size=35.23GiB gpu_memory_utilization=0.95
INFO 12-20 13:03:10 worker.py:232] Memory profiling results: total_gpu_memory=79.32GiB initial_memory_usage=36.96GiB peak_torch_memory=35.38GiB memory_usage_post_profile=39.88GiB non_torch_memory=5.53GiB kv_cache_size=34.45GiB gpu_memory_utilization=0.95
INFO 12-20 13:03:10 distributed_gpu_executor.py:57] # GPU blocks: 28217, # CPU blocks: 3276
INFO 12-20 13:03:10 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 55.11x
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:03:14 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:03:14 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:03:14 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:03:14 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 12-20 13:03:14 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-20 13:03:14 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:03:14 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:03:14 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:03:39 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:03:39 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
INFO 12-20 13:03:39 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:03:39 custom_all_reduce.py:224] Registering 5635 cuda graph addresses
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:03:39 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.94 GiB
INFO 12-20 13:03:39 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.93 GiB
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:03:39 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.94 GiB
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:03:39 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.94 GiB
INFO 12-20 13:03:39 api_server.py:249] vLLM to use /tmp/tmpiz6ike1b as PROMETHEUS_MULTIPROC_DIR
INFO 12-20 13:03:39 launcher.py:19] Available routes are:
INFO 12-20 13:03:39 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET
INFO 12-20 13:03:39 launcher.py:27] Route: /docs, Methods: HEAD, GET
INFO 12-20 13:03:39 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 12-20 13:03:39 launcher.py:27] Route: /redoc, Methods: HEAD, GET
INFO 12-20 13:03:39 launcher.py:27] Route: /health, Methods: GET
INFO 12-20 13:03:39 launcher.py:27] Route: /tokenize, Methods: POST
INFO 12-20 13:03:39 launcher.py:27] Route: /detokenize, Methods: POST
INFO 12-20 13:03:39 launcher.py:27] Route: /v1/models, Methods: GET
INFO 12-20 13:03:39 launcher.py:27] Route: /version, Methods: GET
INFO 12-20 13:03:39 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 12-20 13:03:39 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 12-20 13:03:39 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO:     Started server process [806878]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
INFO 12-20 13:03:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:03:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:04:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:04:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:04:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:04:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:04:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:04:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:05:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:05:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:05:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:05:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:05:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:05:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:06:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:06:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:06:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:06:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:06:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:06:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:07:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:07:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:07:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:07:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:07:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:07:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:08:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:08:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:08:29 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:08:39 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:08:49 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:08:59 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:09:09 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:09:19 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:09:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:09:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:09:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:10:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:10:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:10:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:10:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:10:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:10:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:11:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:11:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:11:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:11:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:11:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:11:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:12:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:12:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:12:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:12:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:12:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:12:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:13:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:13:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:13:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:13:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:13:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:13:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:14:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:14:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:14:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:14:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:14:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:14:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:15:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:15:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:15:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:15:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:15:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:15:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:16:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:16:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:16:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:16:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:16:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:16:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:17:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:17:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:17:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:17:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:17:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:17:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:18:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:18:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:18:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:18:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:18:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:18:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:19:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:19:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:19:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:19:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:19:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:19:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:20:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:20:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:20:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:20:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:20:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:20:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:21:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:21:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:21:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:21:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:21:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:21:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:22:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:22:10 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:22:20 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:22:30 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:22:40 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:22:50 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:23:00 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-20 13:23:01 launcher.py:57] Shutting down FastAPI HTTP server.
INFO 12-20 13:23:01 multiproc_worker_utils.py:133] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=808075)[0;0m INFO 12-20 13:23:01 multiproc_worker_utils.py:240] Worker exiting
[1;36m(VllmWorkerProcess pid=808076)[0;0m INFO 12-20 13:23:01 multiproc_worker_utils.py:240] Worker exiting
[1;36m(VllmWorkerProcess pid=808074)[0;0m INFO 12-20 13:23:01 multiproc_worker_utils.py:240] Worker exiting
[rank0]:[W1220 13:23:04.897763884 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/XYAIFS00/HDD_POOL/nsccgz_zgchen/nsccgz_zgchen_6/veot/get_dataset/miniforge3/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
