#!/bin/bash
set -e

# # === æ˜¾å¡å ç”¨æ£€æµ‹æ¨¡å— ===
# TARGET_GPU=5
# echo "ğŸ” Checking status of GPU ${TARGET_GPU}..."

# while true; do
#     # æŸ¥è¯¢æŒ‡å®š GPU ä¸Šçš„è®¡ç®—è¿›ç¨‹ PID
#     # -i: æŒ‡å®šæ˜¾å¡ID
#     # --query-compute-apps=pid: åªæŸ¥è¯¢è®¡ç®—è¿›ç¨‹çš„PID
#     # --format=csv,noheader: æ ¼å¼åŒ–è¾“å‡ºï¼Œå»æ‰è¡¨å¤´
#     pids=$(nvidia-smi -i $TARGET_GPU --query-compute-apps=pid --format=csv,noheader)

#     # åˆ¤æ–­ pids å­—ç¬¦ä¸²æ˜¯å¦ä¸ºç©º (-z)
#     if [ -z "$pids" ]; then
#         echo "âœ… GPU ${TARGET_GPU} is free! Starting tasks..."
#         break
#     else
#         # è·å–å½“å‰æ—¶é—´
#         now=$(date "+%Y-%m-%d %H:%M:%S")
#         # å¦‚æœä¸ä¸ºç©ºï¼Œè¯´æ˜æœ‰è¿›ç¨‹åœ¨è·‘ï¼Œæ‰“å°æç¤ºå¹¶ç­‰å¾…
#         # echo $pids | tr '\n' ' ' ç”¨äºæŠŠå¤šè¡ŒPIDå˜æˆä¸€è¡Œæ˜¾ç¤º
#         echo "[$now] â³ GPU ${TARGET_GPU} is busy (PIDs: $(echo $pids | tr '\n' ' ')). Waiting 30s..."
#         sleep 30
#     fi
# done
# # ========================

# === é…ç½®åŒºåŸŸ ===
DATASET=$1
SPLIT=${2:-"test"}

# [ä¿®æ”¹ç‚¹]ï¼šå°†è¾“å‡ºè·¯å¾„æ”¹ä¸ºåŒ…å« "retry" çš„æ–°æ–‡ä»¶å¤¹
PRED_PATH="outputs_agent_retry/${DATASET}_${SPLIT}"

mkdir -p $PRED_PATH
LOG_DIR="logs_agent"
mkdir -p $LOG_DIR

# è·å– Python è·¯å¾„
PYTHON_BIN=$(which python)
echo "Using Python: $PYTHON_BIN"


echo "=========================================================="
echo "ğŸš€ Starting vLLM Servers on GPU 1-6..."
echo "=========================================================="

# === 1. å¯åŠ¨ Planner (æ–‡æœ¬æ¨¡å‹) ===
# æ›¿æ¢ä¸º Int4 ç‰ˆæœ¬ï¼ŒTP=1 (å•å¡å³å¯è·‘é£èµ·)
# æ˜¾å¡ï¼šä½¿ç”¨ GPU 1
echo "Starting Planner (Int4) on GPU 1..."
INT4_MODEL="/HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2.5-72B-Instruct-Int4"

CUDA_VISIBLE_DEVICES=1 nohup $PYTHON_BIN -m vllm.entrypoints.openai.api_server \
    --model $INT4_MODEL \
    --served-model-name planner-72b \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.90 \
    --port 8000 \
    --trust-remote-code > $LOG_DIR/planner_server.log 2>&1 &

# === 2. å¯åŠ¨ Grounder (è§†è§‰æ¨¡å‹) on GPU 3,4,5,6 ===
# ç¨³å¥æ–¹æ¡ˆ: TP=4
echo "ğŸš€ Starting Grounder (Qwen2-VL-72B) on GPU 3,4,5,6..."

VL_MODEL_PATH="/HOME/nsccgz_zgchen/nsccgz_zgchen_6/HDD_POOL/veot/model_zoo/Qwen2-VL-72B-Instruct"

CUDA_VISIBLE_DEVICES=2,3,4,5 nohup $PYTHON_BIN -m vllm.entrypoints.openai.api_server \
    --model $VL_MODEL_PATH \
    --served-model-name grounder-vl-72b \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.95 \
    --max-model-len 8192 \
    --port 8001 \
    --trust-remote-code > $LOG_DIR/grounder_server.log 2>&1 &

# === 3. å¾ªç¯ç­‰å¾…æœåŠ¡å°±ç»ª (è¿™å°±æ˜¯ä½ åˆšæ‰ç¼ºå°‘çš„æ­¥éª¤) ===
echo "â³ Waiting for servers to be ready..."

# æ£€æŸ¥ Planner (8000)
while ! nc -z localhost 8000; do
  sleep 5
  echo "Waiting for Planner (8000)..."
  # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦æ„å¤–æ­»äº¡
  if ! pgrep -f "planner-72b" > /dev/null; then
     echo "âŒ Planner died! Check logs: cat $LOG_DIR/planner_server.log"
     exit 1
  fi
done

# æ£€æŸ¥ Grounder (8001)
while ! nc -z localhost 8001; do
  sleep 5
  echo "Waiting for Grounder (8001)..."
  if ! pgrep -f "grounder-vl-72b" > /dev/null; then
     echo "âŒ Grounder died! Check logs: cat $LOG_DIR/grounder_server.log"
     exit 1
  fi
done

echo "âœ… All Servers Ready! Starting Inference..."


# === 4. è¿è¡Œ Python å®¢æˆ·ç«¯ ===
# è¿™é‡Œæˆ‘ä»¬å¼€ 4 ä¸ªè¿›ç¨‹å¹¶å‘è¯·æ±‚ API
CHUNKS=4
CURRENT_DIR=$(pwd)
export PYTHONPATH="$CURRENT_DIR:$PYTHONPATH"

echo "Current PYTHONPATH: $PYTHONPATH"
for IDX in $(seq 0 $((CHUNKS-1))); do
    python videomind/eval/infer_agent_api_new.py \
        --dataset $DATASET \
        --split $SPLIT \
        --pred_path $PRED_PATH \
        --chunk $CHUNKS \
        --index $IDX &
done

wait # ç­‰å¾…æ‰€æœ‰ python è„šæœ¬è·‘å®Œ

echo "ğŸ‰ Evaluation Finished! Results saved to $PRED_PATH"

# === 5. ç»“æŸåæ€æ‰æœåŠ¡å™¨ (å¯é€‰) ===
# pkill -f vllm